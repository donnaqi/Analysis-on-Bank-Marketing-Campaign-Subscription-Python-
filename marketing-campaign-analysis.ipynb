{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-29T22:53:53.386764Z","iopub.execute_input":"2022-01-29T22:53:53.387123Z","iopub.status.idle":"2022-01-29T22:53:53.404794Z","shell.execute_reply.started":"2022-01-29T22:53:53.387086Z","shell.execute_reply":"2022-01-29T22:53:53.403806Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"code","source":"# Import data\ndf = pd.read_csv('/kaggle/input/bank-marketing/bank-additional-full.csv', delimiter=';')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T22:53:55.695146Z","iopub.execute_input":"2022-01-29T22:53:55.696163Z","iopub.status.idle":"2022-01-29T22:53:55.969340Z","shell.execute_reply.started":"2022-01-29T22:53:55.696105Z","shell.execute_reply":"2022-01-29T22:53:55.968522Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Check missing values\ndf.isnull().sum() / df.count()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T22:54:10.074998Z","iopub.execute_input":"2022-01-29T22:54:10.075328Z","iopub.status.idle":"2022-01-29T22:54:10.191656Z","shell.execute_reply.started":"2022-01-29T22:54:10.075293Z","shell.execute_reply":"2022-01-29T22:54:10.190519Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Check target distribution\n# 1. Check weight of each class\n\nprint(df.y.value_counts() / df.y.count())\n\n# 2. Plot the distribution of the target variable\n\ndf.y.value_counts().plot.bar()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T01:00:02.317940Z","iopub.execute_input":"2022-01-30T01:00:02.318273Z","iopub.status.idle":"2022-01-30T01:00:02.532524Z","shell.execute_reply.started":"2022-01-30T01:00:02.318239Z","shell.execute_reply":"2022-01-30T01:00:02.531818Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"# Split the data into charter and numerical variables (exclude y which is the target variable)\nchar = df.drop('y', axis=1).select_dtypes(include=\"object\")\ndec = df.drop('y', axis=1).select_dtypes(exclude=\"object\")","metadata":{"execution":{"iopub.status.busy":"2022-01-29T22:57:50.297165Z","iopub.execute_input":"2022-01-29T22:57:50.297482Z","iopub.status.idle":"2022-01-29T22:57:50.331173Z","shell.execute_reply.started":"2022-01-29T22:57:50.297445Z","shell.execute_reply":"2022-01-29T22:57:50.330171Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Check correlation between numerical variabels\n\ndec.corr()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T22:58:31.639430Z","iopub.execute_input":"2022-01-29T22:58:31.639869Z","iopub.status.idle":"2022-01-29T22:58:31.674509Z","shell.execute_reply.started":"2022-01-29T22:58:31.639832Z","shell.execute_reply":"2022-01-29T22:58:31.673901Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"nr.employed, euribor3m, emp.var.rate and cons.price.idx are highly correlated variables. We will need to be very careful with these variables during the modeling step.","metadata":{}},{"cell_type":"code","source":"# Check statistics of numerical variables\n# Create a table summary\n\nsummary = dec.quantile([0.01, 0.25, 0.5, 0.75, 0.99]).T\nsummary.columns = ['1% percentile', '25% percentile', 'median', '75% percentile', '99% percentile']\nsummary\n\n# 2. Add a column 'mean' to summary table\n\nsummary['mean'] = dec.mean()\nsummary['max'] = dec.max()\nsummary['min'] = dec.min()\nsummary","metadata":{"execution":{"iopub.status.busy":"2022-01-29T23:04:46.040771Z","iopub.execute_input":"2022-01-29T23:04:46.041130Z","iopub.status.idle":"2022-01-29T23:04:46.082403Z","shell.execute_reply.started":"2022-01-29T23:04:46.041098Z","shell.execute_reply":"2022-01-29T23:04:46.081739Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"The 25% percentile of pdays is 999 days which means most of the customers were not contacted before. We will consider using a categorical variable which indicate whether the customer was contacted before or not.","metadata":{"execution":{"iopub.status.busy":"2022-01-29T21:54:35.423710Z","iopub.execute_input":"2022-01-29T21:54:35.423999Z","iopub.status.idle":"2022-01-29T21:54:35.442735Z","shell.execute_reply.started":"2022-01-29T21:54:35.423969Z","shell.execute_reply":"2022-01-29T21:54:35.441276Z"}}},{"cell_type":"code","source":"# Check unique values of each categorical variable\n# Use unique() function with a for loop\n\nfor col in df.columns:\n    print()\n    if df[col].dtype == 'object':\n        print(f'Name of Column is: {col} and unique values are: {df[col].unique()}')","metadata":{"execution":{"iopub.status.busy":"2022-01-29T23:07:25.756902Z","iopub.execute_input":"2022-01-29T23:07:25.757266Z","iopub.status.idle":"2022-01-29T23:07:25.807555Z","shell.execute_reply.started":"2022-01-29T23:07:25.757224Z","shell.execute_reply":"2022-01-29T23:07:25.806911Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"There are some 'unknown' values. Since those are categorical variables, we can treat them as individual classes.","metadata":{}},{"cell_type":"code","source":"# add \"pdays_char\" column to df dataset to indicate whether the customer was contacted before or not. \n\ndf['pdays_char'] = df.apply(lambda x: 1 if x.pdays != 999 else 0, axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T23:12:16.961598Z","iopub.execute_input":"2022-01-29T23:12:16.962161Z","iopub.status.idle":"2022-01-29T23:12:17.776256Z","shell.execute_reply.started":"2022-01-29T23:12:16.962119Z","shell.execute_reply":"2022-01-29T23:12:17.775369Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Create a modeling dataset\n# Drop some variables that are irrelevant to the models.\n# Create a dataset y that contains only target variable\n\nvars_to_drop = ['pdays', 'month', 'day_of_week']\ndf_clean = df.drop(vars_to_drop, axis=1)\ndf_model = pd.get_dummies(df_clean.drop('y', axis=1), drop_first=True)\ndf_y = df['y'].apply(lambda x: 1 if x == 'yes' else 0)\ndf_y","metadata":{"execution":{"iopub.status.busy":"2022-01-29T23:42:19.102875Z","iopub.execute_input":"2022-01-29T23:42:19.103174Z","iopub.status.idle":"2022-01-29T23:42:19.204573Z","shell.execute_reply.started":"2022-01-29T23:42:19.103144Z","shell.execute_reply":"2022-01-29T23:42:19.203698Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# Normalize the data by deduct the mean and then divide by standard deviation of the data\n\ndf_model = (df_model - df_model.mean()) / df_model.std()","metadata":{"execution":{"iopub.status.busy":"2022-01-29T23:42:57.634835Z","iopub.execute_input":"2022-01-29T23:42:57.635196Z","iopub.status.idle":"2022-01-29T23:42:57.671196Z","shell.execute_reply.started":"2022-01-29T23:42:57.635159Z","shell.execute_reply":"2022-01-29T23:42:57.670505Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"## Modeling","metadata":{}},{"cell_type":"code","source":"# Create train, test plit\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df_model, df_y, test_size=0.33, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T23:42:59.867756Z","iopub.execute_input":"2022-01-29T23:42:59.868219Z","iopub.status.idle":"2022-01-29T23:42:59.913000Z","shell.execute_reply.started":"2022-01-29T23:42:59.868185Z","shell.execute_reply":"2022-01-29T23:42:59.911806Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)\nprint(X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T23:43:01.940957Z","iopub.execute_input":"2022-01-29T23:43:01.941284Z","iopub.status.idle":"2022-01-29T23:43:01.946691Z","shell.execute_reply.started":"2022-01-29T23:43:01.941252Z","shell.execute_reply":"2022-01-29T23:43:01.945749Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"### Logistic regression model - Lasso\nSince previously we have identified some highly correlated variables, we will introduce l1 regularization here to deal with multicollinearity.","metadata":{}},{"cell_type":"code","source":"# Define the logistic regression model. \nfrom sklearn import linear_model\nclf = linear_model.Lasso(alpha=0.1)\nclf.fit(X_train, y_train)\nprint(clf.coef_)","metadata":{"execution":{"iopub.status.busy":"2022-01-29T23:43:21.066894Z","iopub.execute_input":"2022-01-29T23:43:21.067195Z","iopub.status.idle":"2022-01-29T23:43:21.097974Z","shell.execute_reply.started":"2022-01-29T23:43:21.067164Z","shell.execute_reply":"2022-01-29T23:43:21.096989Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"new = pd.DataFrame([X_train.columns, clf.coef_])\nnew.T","metadata":{"execution":{"iopub.status.busy":"2022-01-29T23:50:23.319159Z","iopub.execute_input":"2022-01-29T23:50:23.319500Z","iopub.status.idle":"2022-01-29T23:50:23.338596Z","shell.execute_reply.started":"2022-01-29T23:50:23.319467Z","shell.execute_reply":"2022-01-29T23:50:23.337334Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# Predict on both training set and testing set\n\ny_train_predict = clf.predict(X_train) > 0.5\ny_test_predict = clf.predict(X_test) > 0.5","metadata":{"execution":{"iopub.status.busy":"2022-01-29T23:57:47.153381Z","iopub.execute_input":"2022-01-29T23:57:47.153665Z","iopub.status.idle":"2022-01-29T23:57:47.169111Z","shell.execute_reply.started":"2022-01-29T23:57:47.153635Z","shell.execute_reply":"2022-01-29T23:57:47.167816Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"# Calculate precision score for each data \nfrom sklearn.metrics import precision_score\nprint(precision_score(y_train, y_train_predict, average='macro'))\nprint(precision_score(y_test, y_test_predict, average='macro'))","metadata":{"execution":{"iopub.status.busy":"2022-01-30T00:00:41.927717Z","iopub.execute_input":"2022-01-30T00:00:41.928105Z","iopub.status.idle":"2022-01-30T00:00:41.952486Z","shell.execute_reply.started":"2022-01-30T00:00:41.928068Z","shell.execute_reply":"2022-01-30T00:00:41.951446Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"### Decision tree model","metadata":{}},{"cell_type":"code","source":"# Build the decision tree model. \nfrom sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier(random_state=0, max_depth=3)\nclf.fit(X_train, y_train)\nprint(clf.feature_importances_)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T00:34:17.385944Z","iopub.execute_input":"2022-01-30T00:34:17.386533Z","iopub.status.idle":"2022-01-30T00:34:17.445822Z","shell.execute_reply.started":"2022-01-30T00:34:17.386496Z","shell.execute_reply":"2022-01-30T00:34:17.444924Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"new = pd.DataFrame([X_train.columns, clf.feature_importances_])\nnew.T","metadata":{"execution":{"iopub.status.busy":"2022-01-30T00:34:19.220761Z","iopub.execute_input":"2022-01-30T00:34:19.221083Z","iopub.status.idle":"2022-01-30T00:34:19.237455Z","shell.execute_reply.started":"2022-01-30T00:34:19.221044Z","shell.execute_reply":"2022-01-30T00:34:19.236549Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"y_train_predict = clf.predict(X_train)\ny_test_predict = clf.predict(X_test)\nprint(y_train_predict)","metadata":{"execution":{"iopub.status.busy":"2022-01-30T00:34:27.093701Z","iopub.execute_input":"2022-01-30T00:34:27.094384Z","iopub.status.idle":"2022-01-30T00:34:27.109952Z","shell.execute_reply.started":"2022-01-30T00:34:27.094349Z","shell.execute_reply":"2022-01-30T00:34:27.109046Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"# Calculate precision, recall, f1 and accuracy score for each data\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\nprint('This is the precision of the training dataset:', precision_score(y_train, y_train_predict, average='macro'))\nprint('This is the precision of the testing dataset:', precision_score(y_test, y_test_predict, average='macro'))\nprint('This is the recall of the training dataset:', recall_score(y_train, y_train_predict, average='macro'))\nprint('This is the recall of the testing dataset:', recall_score(y_test, y_test_predict, average='macro'))\nprint('This is the f1 of the training dataset:', f1_score(y_train, y_train_predict, average='macro'))\nprint('This is the f1 of the testing dataset:', f1_score(y_test, y_test_predict, average='macro'))\nprint('This is the accuracy of the training dataset:', accuracy_score(y_train, y_train_predict))\nprint('This is the accuracy of the testing dataset:', accuracy_score(y_test, y_test_predict))","metadata":{"execution":{"iopub.status.busy":"2022-01-30T00:34:28.516010Z","iopub.execute_input":"2022-01-30T00:34:28.516418Z","iopub.status.idle":"2022-01-30T00:34:28.573008Z","shell.execute_reply.started":"2022-01-30T00:34:28.516389Z","shell.execute_reply":"2022-01-30T00:34:28.571841Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nprint('This is the confusion matrix of the training dataset:', confusion_matrix(y_train, y_train_predict))\nprint('This is the confusion matrix of the testing dataset:', confusion_matrix(y_test, y_test_predict))","metadata":{"execution":{"iopub.status.busy":"2022-01-30T00:34:30.693078Z","iopub.execute_input":"2022-01-30T00:34:30.693427Z","iopub.status.idle":"2022-01-30T00:34:30.749289Z","shell.execute_reply.started":"2022-01-30T00:34:30.693382Z","shell.execute_reply":"2022-01-30T00:34:30.748349Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(clf, X_train, y_train)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T00:34:32.327005Z","iopub.execute_input":"2022-01-30T00:34:32.327335Z","iopub.status.idle":"2022-01-30T00:34:32.591213Z","shell.execute_reply.started":"2022-01-30T00:34:32.327300Z","shell.execute_reply":"2022-01-30T00:34:32.590520Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(clf, X_test, y_test)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T00:34:34.343765Z","iopub.execute_input":"2022-01-30T00:34:34.344263Z","iopub.status.idle":"2022-01-30T00:34:34.571715Z","shell.execute_reply.started":"2022-01-30T00:34:34.344200Z","shell.execute_reply":"2022-01-30T00:34:34.571097Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"markdown","source":"## Feature analysis","metadata":{}},{"cell_type":"code","source":"# Plot some varibales with large variable importance. \ndf.groupby('y').mean().duration.plot.bar()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T00:34:36.724117Z","iopub.execute_input":"2022-01-30T00:34:36.724590Z","iopub.status.idle":"2022-01-30T00:34:36.917533Z","shell.execute_reply.started":"2022-01-30T00:34:36.724541Z","shell.execute_reply":"2022-01-30T00:34:36.916605Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"df1 = df[df_model['nr.employed'] <= -1.099] \ndf1.groupby('y').mean()['nr.employed'].plot.bar()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T00:44:18.817188Z","iopub.execute_input":"2022-01-30T00:44:18.818058Z","iopub.status.idle":"2022-01-30T00:44:19.025002Z","shell.execute_reply.started":"2022-01-30T00:44:18.818012Z","shell.execute_reply":"2022-01-30T00:44:19.024127Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"df2 = df[df_model['nr.employed'] > -1.099] \ndf2.groupby('y').mean()['nr.employed'].plot.bar()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T00:44:23.384063Z","iopub.execute_input":"2022-01-30T00:44:23.384512Z","iopub.status.idle":"2022-01-30T00:44:23.576409Z","shell.execute_reply.started":"2022-01-30T00:44:23.384474Z","shell.execute_reply":"2022-01-30T00:44:23.575792Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"from sklearn import tree\nplt.figure(figsize=(100,100))\ntree.plot_tree(clf)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-30T00:37:25.433985Z","iopub.execute_input":"2022-01-30T00:37:25.434347Z","iopub.status.idle":"2022-01-30T00:37:29.100870Z","shell.execute_reply.started":"2022-01-30T00:37:25.434310Z","shell.execute_reply":"2022-01-30T00:37:29.100069Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion","metadata":{}},{"cell_type":"code","source":"# Draw conclusion based on identified important variables","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on our model output, the most important variable that impacts whether clients will subscribe the bank term deposit product. Our plot has shown that there is a positive correlation between duration and the subscription rate, the longer the duration of the last contact, the more likely the clients will subscribe the bank term deposit product. Even though the decision tree model consider the number of employees as an important variable, we do not see that there is any correlation between the subscription rate. We also consider the logistic regression model; however, the poor performance of the model indicates that logistic regression may not be a good fit for the data set and the conclusion drew from the logistic regression may not be reliable. Overall, our recommendation is that we could keep the call longer for selling the products to clients.","metadata":{}}]}